{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISPR-RNA-Editing Model Training and Evaluation\n",
    "Harsh D. <br/>\n",
    "Updated on 10/19/20\n",
    "#### Notebook to train models based on the sequence and the edit rates contained in the metadata for each sample in the .fasta file. Can also create temporary .npz files for faster loading so the edit rates and sequences don't need to be extracted every time.\n",
    "temp files stored in tmp/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations, utils\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "tp = \"ABE\" #for cytidine deaminase base editor; \"ABE\" for adenine deaminase base editor\n",
    "DATA_DIR = \"crispr/data/Dataset/\" #\"Path to .fasta files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "def seq_to_onehot(seq): \n",
    "    seq = seq.strip()\n",
    "    values = list(seq+\"ACTG\") \n",
    "    label_encoder = LabelEncoder();\n",
    "    integer_encoded = label_encoder.fit_transform(values);\n",
    "    onehot_encoder = OneHotEncoder(sparse=False);\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1);\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded); \n",
    "    return onehot_encoded[:-4] \n",
    "def editdata(line):\n",
    "    data = line.split(\"_\")\n",
    "    return float(data[3])\n",
    "def getData(chromosome):\n",
    "    start = time.time()\n",
    "    out = list()\n",
    "    inp = list()\n",
    "    with open(DATA_DIR+'{}.chr{}.fasta'.format(exp, chromosome),'rt') as f:\n",
    "        for line in tqdm(f):\n",
    "            if line[0:1] == \">\":\n",
    "                y = editdata(line)\n",
    "                out.append(y)\n",
    "            else:\n",
    "                x = seq_to_onehot(line)\n",
    "                assert x.shape[1] == 4\n",
    "                inp.append(x)\n",
    "    print(time.time()-start)\n",
    "    X = np.stack(inp)\n",
    "    Y = np.stack(out)\n",
    "    print(Y.shape[0])\n",
    "    Y = Y.reshape(Y.shape[0], 1)\n",
    "    np.savez_compressed(\"tmp/{}.chr{}\".format(exp, chromosome),x = X, y = Y)\n",
    "def load_data(chromosomes):\n",
    "    start = time.time()\n",
    "    num_workers = len(chromosomes)\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        res = pool.map(getData, chromosomes)\n",
    "    print(time.time()-start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should prob. run it the first time with rebuild and rebuildTst set to true to generate the temp files but then set them to false to skip the process for later runs. If you get a file not found, you have not generated the temp file yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "rebuild = False # Whether or not to rebuild training temp files or just use the ones already generated\n",
    "rebuildTst = False # Whether or not to rebuild testing temp files or just use the ones already generated\n",
    "\n",
    "test_set = [\"20\", \"21\", \"22\", \"X\"] # The chromosomes that will be used for testing the model's performance\n",
    "train_set = [str(x) for x in range(1, 20)] # The chromosomes that will be used for training the model.\n",
    "val_set = test_set\n",
    "print(train_set)\n",
    "print(val_set)\n",
    "print(test_set)\n",
    "\n",
    "Xtr = list()\n",
    "Ytr = list()\n",
    "\n",
    "train = [\"156B\"] # The experiment from which the the training chromosomes will be taken\n",
    "validation = [\"157B\"] # The experiment from which the the validation chromosomes will be taken\n",
    "test = [\"158B\"] # The experiment from which the testing chromosomes will be taken\n",
    " \n",
    "for experiment in train:\n",
    "    exp = experiment\n",
    "    if rebuild is True:\n",
    "        load_data(train_set)\n",
    "    for x in train_set:\n",
    "        lmn = np.load(\"tmp/{}.chr{}.npz\".format(exp, x))\n",
    "        Xtr.append(lmn['x'])\n",
    "        Ytr.append(lmn['y'])\n",
    "\n",
    "X_train = np.concatenate(Xtr, axis=0)\n",
    "Y_train = np.concatenate(Ytr, axis=0)\n",
    "print(\"TOT: {}\".format(X_train.shape[0]))\n",
    "\n",
    "Xval = list()\n",
    "Yval = list()\n",
    "\n",
    "for experiment in validation:\n",
    "    exp = experiment\n",
    "    if rebuild is True:\n",
    "        load_data(val_set)\n",
    "    for x in val_set:\n",
    "        lmn = np.load(\"tmp/{}.chr{}.npz\".format(exp, x))\n",
    "        Xval.append(lmn['x'])\n",
    "        Yval.append(lmn['y'])\n",
    "        \n",
    "X_val = np.concatenate(Xval, axis=0)\n",
    "Y_val = np.concatenate(Yval, axis=0)\n",
    "print(\"TOT: {}\".format(X_val.shape[0]))    \n",
    "\n",
    "X_tst = list()\n",
    "Y_tst = list()\n",
    "for experiment in test:\n",
    "    exp = experiment\n",
    "    if rebuildTst is True:\n",
    "        load_data(test_set)\n",
    "    for x in test_set:\n",
    "            lmn = np.load(\"tmp/{}.chr{}.npz\".format(exp, x))\n",
    "            X_tst.append(lmn['x'])\n",
    "            Y_tst.append(lmn['y'])\n",
    "X_test = np.concatenate(X_tst, axis=0)\n",
    "Y_test = np.concatenate(Y_tst, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"\\_dz\" variables were supposed to have only non-zero edit rate sequences and values but this was scrapped and the whole dataset is being used. This is the reason for the seemingly useless name change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dz = X_train\n",
    "Y_train_dz = Y_train\n",
    "print(X_train_dz.shape)\n",
    "print(Y_train_dz.shape)\n",
    "X_val_dz = X_val\n",
    "Y_val_dz = Y_val\n",
    "print(X_val_dz.shape)\n",
    "print(Y_val_dz.shape)\n",
    "X_test_dz = X_test\n",
    "Y_test_dz = Y_test\n",
    "print(X_test_dz.shape)\n",
    "print(Y_test_dz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple data generator to give reversed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow import keras\n",
    "class SeqDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.dim = x_set.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.x))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __on_epoch_end__(self):\n",
    "        np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = np.empty((2*self.batch_size, self.dim, 4))\n",
    "        Y = np.empty((2*self.batch_size))\n",
    "        for i, ID in enumerate(self.indexes[self.batch_size*index:min((self.batch_size)*(index+1), len(self.x))]):\n",
    "                X[i,] = self.x[ID]\n",
    "                X[self.batch_size+i,] = np.flipud(self.x[ID])\n",
    "                Y[i] = self.y[ID]\n",
    "                Y[self.batch_size+i,] = self.y[ID]\n",
    "        return (X, Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets up the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations, Model, backend\n",
    "from tensorflow.keras.layers import Input, Conv1D, ReLU, BatchNormalization,\\\n",
    "                                    Add, Activation, MaxPooling1D, Flatten, Dense, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "n = 10\n",
    "def build_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "        layers.Input(shape=(2*n+1,4)),\n",
    "        layers.Conv1D(128, 16, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv1D(128, 8, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool1D(2, padding='same'),\n",
    "        layers.Conv1D(64, 12, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool1D(2, padding='same'),\n",
    "        layers.Conv1D(64, 8, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv1D(64, 8, padding='same', activation='relu'),\n",
    "        layers.MaxPool1D(4, padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model\n",
    "def build_reg_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "        layers.Input(shape=(2*n+1,4)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.RMSprop()\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model\n",
    "def rb(x, n, w, dout):  \n",
    "    y = Conv1D(n, w, padding='same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dropout(dout)(y)\n",
    "    out = Add()([x, y])\n",
    "    return out\n",
    "def resnet():\n",
    "    dout = 0.1\n",
    "    lsr = 32\n",
    "    lsr2 = 64\n",
    "    lsr3 = 64\n",
    "    inputs = keras.Input(shape=(2*n+1, 4))\n",
    "    x = Conv1D(lsr, 1, padding='same')(inputs)\n",
    "    x = rb(x, lsr, 3, dout)\n",
    "    x = rb(x, lsr, 3, dout)\n",
    "    x = rb(x, lsr, 3, dout)\n",
    "    x = MaxPooling1D(4, padding = 'same')(x)\n",
    "    x = Conv1D(lsr2, 1, padding='same')(x)\n",
    "    x = rb(x, lsr2, 7, dout)\n",
    "    x = rb(x, lsr2, 7, dout)\n",
    "    x = MaxPooling1D(4, padding = 'same')(x)\n",
    "    x = Conv1D(lsr3, 1, padding='same')(x)\n",
    "    x = rb(x, lsr3, 7, dout)\n",
    "    x = MaxPooling1D(4, padding = 'same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(lsr, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, output)\n",
    "    optimizer = tf.keras.optimizers.RMSprop()\n",
    "    model.compile(loss=\"mse\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def build_func_model():\n",
    "    dout = 0.1\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "        layers.Input(shape=(2*n+1,4)),\n",
    "        layers.Conv1D(48, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),\n",
    "        layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),  \n",
    "        layers.Conv1D(100, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),  \n",
    "        layers.MaxPooling1D(2, padding='same'),\n",
    "        layers.Conv1D(150, 7, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),\n",
    "        layers.Conv1D(300, 7, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),\n",
    "        layers.MaxPooling1D(3, padding='same'),\n",
    "        layers.Conv1D(200, 7, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),\n",
    "        layers.MaxPooling1D(4, padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dout),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "tp = \"ABE\"\n",
    "now = str(int(time.time()))\n",
    "\n",
    "checkpoint_filepath = 'models/ConvNet-{}-{}-{}.h5'.format(2*n+1, tp, now)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "batch_size = 1024\n",
    "sdg = SeqDataGenerator(X_train_dz[:,500-n:501+n,:], Y_train_dz, batch_size)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "history = model.fit(x=sdg, epochs=20, validation_data=(X_val_dz[:,500-n:501+n,:], Y_val_dz), callbacks=[model_checkpoint_callback, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "model.summary()\n",
    "print(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "Y_pred = model.predict(X_test_dz[:,500-n:501+n,:], batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred.shape)\n",
    "print(Y_test_dz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_dz = Y_test_dz.reshape(Y_test_dz.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def getSpearman(arr1, arr2):    \n",
    "    return stats.spearmanr(arr1.astype(\"float64\"), arr2.astype(\"float64\"))\n",
    "import scipy\n",
    "print(\"Spearman Coeff:\\t\"+str(getSpearman(Y_pred, Y_test_dz)[0]))\n",
    "print(\"Pearson Coeff:\\t\"+str(scipy.stats.pearsonr(Y_pred.flatten(), Y_test_dz.flatten())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "import matplotlib.pyplot as plt\n",
    "nbins=100\n",
    "x = Y_pred.flatten()\n",
    "y = Y_test_dz.flatten()\n",
    "k = kde.gaussian_kde([x,y])\n",
    "xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\n",
    "zi = k(np.vstack([xi.flatten(), yi.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib\n",
    "plt.style.use(['dark_background'])\n",
    "exp = \"158B Chrom 20-X\"\n",
    "plt.axis('square')\n",
    "plt.ylabel(\"Observed from Experiment {}\".format(exp))\n",
    "plt.xlabel(\"Predicted from Experiment {}\".format(exp))\n",
    "plt.title(\"ABE ResNet(Seq. length = {})\".format(2*n+1))\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.pcolormesh(xi, yi, zi.reshape(xi.shape),cmap=plt.cm.inferno, norm=matplotlib.colors.LogNorm(vmin=1e-2), snap=False)\n",
    "plt.show()\n",
    "import scipy\n",
    "print(\"Spearman Coeff:\\t\"+str(getSpearman(Y_pred, Y_test_dz)[0]))\n",
    "print(\"Pearson Coeff:\\t\"+str(scipy.stats.pearsonr(Y_pred.flatten(), Y_test_dz.flatten())[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.15.2]",
   "language": "python",
   "name": "conda-env-tf1.15.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
