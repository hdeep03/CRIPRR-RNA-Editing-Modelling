{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from src.models.conv_model import build_model as build_conv_model\n",
    "from src.data_loader.RNASeqStructLoader import RNASeqStructDataGenerator \n",
    "from src.models.conv_model import correlation_coefficient_loss, pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Flatten, BatchNormalization, MaxPool1D, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "import numpy\n",
    "from src.models.conv_model import build_model as build_conv_model\n",
    "import tensorflow as tf\n",
    "from src.evaluator.evaluator import Evaluator\n",
    "from src.models.conv_model import correlation_coefficient_loss, pearson_r\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from tqdm.keras import TqdmCallback\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "dependencies = {\n",
    "    'correlation_coefficient_loss': correlation_coefficient_loss,\n",
    "    'pearson_r': pearson_r\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    print(params)\n",
    "    lr = params['lr']\n",
    "    conv_layers_1 = int(params['conv_layers_1'])\n",
    "    dropout = params['dropout']\n",
    "    kernel_size_1 = int(params['kernel_size_1'])\n",
    "    filters_1 = int(params['filters_1'])\n",
    "    kernel_size_2 = int(params['kernel_size_2'])\n",
    "    filters_2 = int(params['filters_2'])\n",
    "    conv_layers_2 = int(params['conv_layers_2'])\n",
    "    dense_layers = int(params['dense_layers'])\n",
    "    dense_layer_nodes = int(params['dense_layer_nodes'])\n",
    "    pool1 = int(params['pool1'])\n",
    "    pool2 = int(params['pool2'])\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(101,7)))\n",
    "    \n",
    "    for x in range(0, conv_layers_1):\n",
    "        model.add(Conv1D(filters=filters_1, kernel_size=kernel_size_1, padding='same'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Conv1D(filters=filters_1, kernel_size=kernel_size_2, strides=pool1, padding='same')) \n",
    "               \n",
    "    for x in range(0, conv_layers_2):\n",
    "        model.add(Conv1D(filters=filters_2, kernel_size=kernel_size_2, padding='same'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "     \n",
    "    model.add(Conv1D(filters=filters_2, kernel_size=kernel_size_2, strides=pool2, padding='same')) \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for x in range(0, dense_layers):\n",
    "        model.add(Dense(dense_layer_nodes, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=math.pow(10, lr))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_crossentropy', 'mse', pearson_r])\n",
    "    \n",
    "    checkpoint_filepath = 'models/cDNA-ABE/logs/model_ckpt/{}.h5'.format(time())\n",
    "\n",
    "    csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "    model_ckpt = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    train_generator = RNASeqStructDataGenerator(\"data/processed/cDNA-ABE/train_data.hdf5\", 1024)\n",
    "    validation_generator = RNASeqStructDataGenerator(\"data/processed/cDNA-ABE/validation_data.hdf5\", 256)\n",
    "    \n",
    "    log = open(\"models/cDNA-ABE/logs/{}.log\".format(time()), \"a\")\n",
    "    sys.stdout = log\n",
    "    print(model.summary())\n",
    "    history = model.fit(x=train_generator, epochs=6, validation_data=validation_generator, callbacks=[model_ckpt, csv_logger], use_multiprocessing=True, workers=5, verbose=2)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    log.close();\n",
    "    del model\n",
    "    min_loss = min(history.history['val_loss'])\n",
    "    tf.reset_default_graph()\n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# space = {\n",
    "#     'lr': hp.uniform('lr', -4.5, -2),\n",
    "#     'dropout': hp.uniform('dropout', 0.05, 0.3),\n",
    "#     'conv_layers_1': hp.quniform('conv_layers_1', 2, 5, 1),\n",
    "#     'kernel_size_1': hp.quniform('kernel_size_1', 4, 64, 1), \n",
    "#     'filters_1': hp.quniform('filters_1', 32, 192, 1), \n",
    "#     'kernel_size_2': hp.quniform('kernel_size_2', 4, 64, 1),\n",
    "#     'filters_2': hp.quniform('filters_2', 16, 96, 1),\n",
    "#     'conv_layers_2': hp.quniform('conv_layers_2', 2, 5, 1),\n",
    "#     'dense_layers': hp.quniform('dense_layers', 0, 3, 1),\n",
    "#     'dense_layer_nodes': hp.quniform('dense_layer_nodes', 16, 64, 1),\n",
    "#     'pool1': hp.quniform('pool1', 1, 3, 1),\n",
    "#     'pool2': hp.quniform('pool2', 1, 3, 1)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin\n",
    "import numpy as np\n",
    "def test_objective(params):\n",
    "    return math.pow(float(params['x']),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "import pickle\n",
    "import os\n",
    "from hyperopt import trials_from_docs\n",
    "def regenerateAcc():\n",
    "    accumulator = Trials()\n",
    "    trs = list()\n",
    "    \n",
    "    for file in os.listdir(\"models/cDNA-ABE/bayesian_opt/trials\"):\n",
    "        if file.endswith(\".p\"):\n",
    "            trs.append(pickle.load(open(os.path.join(\"models/cDNA-ABE/bayesian_opt/trials\", file), \"rb\")))\n",
    "    \n",
    "    for trial in trs:\n",
    "        for a in list(trial):\n",
    "            if not a in accumulator:\n",
    "                accumulator = trials_from_docs([a]+list(accumulator))\n",
    "        \n",
    "    pickle.dump(accumulator, open(\"models/cDNA-ABE/bayesian_opt/accumulator.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from hyperopt import tpe, hp, fmin\n",
    "from hyperopt import Trials\n",
    "def run_trials(n):\n",
    "\n",
    "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 0\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"models/cDNA-ABE/bayesian_opt/accumulator.p\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        space = pickle.load(open(\"models/cDNA-ABE/bayesian_opt/space.p\", \"rb\"))\n",
    "        print(\"Found saved Search Space! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        print(\"ISSUE WITH LOADING SAVED TRIALS\")\n",
    "        return;\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"models/cDNA-ABE/bayesian_opt/trials/trial-{}.p\".format(n), \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    regenerateAcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerateAcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Found saved Search Space! Loading...\n",
      "Rerunning from 106 trials to 107 (+1) trials\n",
      "{'conv_layers_1': 5.0, 'conv_layers_2': 5.0, 'dense_layer_nodes': 53.0, 'dense_layers': 1.0, 'dropout': 0.07239445191120981, 'filters_1': 183.0, 'filters_2': 89.0, 'kernel_size_1': 4.0, 'kernel_size_2': 26.0, 'lr': -4.064634750691902, 'pool1': 1.0, 'pool2': 3.0}\n",
      " 99%|█████████▉| 106/107 [00:00<00:00, 337.76trial/s, best loss=?]WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "while True:\n",
    "    run_trials(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.15.2]",
   "language": "python",
   "name": "conda-env-tf1.15.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
